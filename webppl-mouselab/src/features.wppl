// Defines features for linear approximation of the optimal Q function.

/* The state is represented as a tree. Each element in the tree is one of
   - a number, a previously observed reward
   - 'hidden', an unobserved node
   - 'observe', a node that will hypothetically be observed
*/

var OBSERVE = '__OBSERVE__';

/* The subjective reward function returns a sample from the
   distribution of the expected reward of the root of the given tree.
   that is about to be observed, the expected observationValue will be
   exactly the observationValue that is observed, and thus the future 
   expected reward has the same distribution as the current 
   unknown reward.
*/
var subjectiveReward = function(tree) {
  // tree is a 2-item list [val, [c1, c2...]] where each child c is also a tree.
  var val = tree[0];
  val == UNKNOWN ? expectation(globalStore.reward) :
  val == OBSERVE ? sample(globalStore.reward) :
  val  // already observed
};

/* The distribution over the expected value of the best path through 
   a tree, where the expectation is conditioned on the outcome of
   some yet-to-be-made observations.
*/
var observationValue = dp.cache(function(tree, params) {
    var params = extend({method: 'enumerate'}, params)
    Infer(extend(params, {model() {
      var bestChildVal = (
        tree[1].length == 0 ?
        0 :
        _.max(map(function(child) {sample(observationValue(child, params))},
                  tree[1]))
      );
      subjectiveReward(tree) + bestChildVal
    }}))
});  

var expectedObservationValue = function(tree, params) {
  // console.log('EOV', JSON.stringify(tree))
  var startTime = Date.now()
  var result = expectation(observationValue(tree, params))
  // console.log('observationValue:', Math.round(result * 100) / 100,
  //             '  time:', Date.now() - startTime)
  result
};

/* Alternative representation of the state, used to compute observationValue. */
var stateTree = function(state) {
  var s = state;
  [s[0], [
    // [s[1], [[s[5], [[s[9], []], [s[10], []]]]]],
    // [s[2], [[s[6], [[s[11], []], [s[12], []]]]]],
    // [s[3], [[s[7], [[s[13], []], [s[14], []]]]]],
    // [s[4], [[s[8], [[s[15], []], [s[16], []]]]]]
    [s[1], [[s[2], [[s[3], []], [s[4], []]]]]],
    [s[5], [[s[6], [[s[7], []], [s[8], []]]]]],
    [s[9], [[s[10], [[s[11], []], [s[12], []]]]]],
    [s[13], [[s[14], [[s[15], []], [s[16], []]]]]]
  ]]
}

var actionGroups = [
  [1, 2, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4],
  [5, 6, 7, 8], [5, 6, 7, 8], [5, 6, 7, 8], [5, 6, 7, 8], [5, 6, 7, 8],
  [9, 10, 11, 12], [9, 10, 11, 12], [9, 10, 11, 12], [9, 10, 11, 12], [9, 10, 11, 12],
  [13, 14, 15, 16], [13, 14, 15, 16], [13, 14, 15, 16], [13, 14, 15, 16], [13, 14, 15, 16]
]

var obsTree = function(state, toObserve) {
  stateTree(mapIndexed(function(i, r) {
    (_.includes(toObserve, i) && r == UNKNOWN) ? OBSERVE : r
  }, state))
}

var termValue = cache(function(state) {
  expectedObservationValue(stateTree(state))
});

var VOC_1 = function(state, action) {
  expectedObservationValue(obsTree(state, [action])) - termValue(state)
}

var VPI_full = cache(function(state) {
  expectedObservationValue(obsTree(state, nodes)) - termValue(state)
});

var VPI_action = function(state, action) {
  var obs = actionGroups[action];
  expectedObservationValue(obsTree(state, obs)) - termValue(state)
};

var dot = function(x, y) {
  sum(map2(function(x, y) {
    x * y
  }, x, y))
  
};

var makeQ_meta = function(weights) {
  var Q_meta = cache(function(state, action) {
    // var weights = [30, 30, 18.52, -2.57];
    action == TERM_ACTION ?
    termValue(state) :
    dot(weights, [globalStore.cost,
                  VOC_1(state, action),
                  VPI_action(state, action),
                  VPI_full(state)])
  });
  return Q_meta  
};

var Q_meta = makeQ_meta([20.27524, -0.86362, 27.71059, 16.14729]);


var V_meta = function(state) {
  // console.log('V_meta', JSON.stringify(state))
  var best = maxWith(function(action) {Q_meta(state, action)},
                     actions(state))
  console.log('best action', best[0], 'value', best[1])
  return best[1]
};

var calculatePR = function(arg) {
  Q_meta(arg.state, arg.action) - V_meta(arg.state)
}
