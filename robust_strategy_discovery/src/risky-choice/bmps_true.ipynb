{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/amehta/virtualenvs/mouselab/lib/python3.6/site-packages/sklearn/ensemble/weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from mouselabdiscrete import NewMouselabEnv\n",
    "from evaluation import *\n",
    "from distributions import sample, expectation, Normal, Categorical, Mixture, PiecewiseUniform\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "bins = [(-1, -0.8),\n",
    " (-0.8, -0.6),\n",
    " (-0.6, -0.4),\n",
    " (-0.4, -0.2),\n",
    " (-0.2, 0.0),\n",
    " (0.0, 0.2),\n",
    " (0.2, 0.4),\n",
    " (0.4, 0.6),\n",
    " (0.6, 0.8),\n",
    " (0.8, 1.0)]\n",
    "\n",
    "stock_id = 0\n",
    "\n",
    "stockfile = open(\"stock_dists.pkl\", 'rb')\n",
    "stock_dists = pickle.load(stockfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_probs = stock_dists[sorted(stock_dists.keys())[stock_id]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_datapoints = 10000\n",
    "num_epochs = 5000\n",
    "num_episodes = 1000\n",
    "num_test_episodes = 1000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "gambles = 7\n",
    "attributes = 4\n",
    "scaledist = 100\n",
    "scalecost = 200\n",
    "#low_stakes = Normal((0.25+0.01)/2, 0.3*(0.25-0.01))\n",
    "reward = PiecewiseUniform(bins, true_probs)*scaledist\n",
    "testreward = reward\n",
    "cost=0.01*scalecost\n",
    "\n",
    "alpha = 0.15\n",
    "\n",
    "max_action = (gambles + 1)*attributes + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewardfilename = 'truerewards/bmps' + str(scalecost) + '_' + str(stock_id) + '.npy'\n",
    "myopicfilename = 'truerewards/mvoc' + str(scalecost) + '_' + str(stock_id) + '.npy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BMPS_Approximator(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(BMPS_Approximator, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.network = nn.Sequential(\n",
    "                            nn.Linear(input_size, input_size//2),\n",
    "                            nn.ReLU(),\n",
    "                            nn.Linear(input_size // 2, input_size // 4),\n",
    "                            nn.ReLU(),\n",
    "                            nn.Linear(input_size // 4, input_size//8),\n",
    "                            nn.ReLU(),\n",
    "                            nn.Linear(input_size // 8, output_size)\n",
    "                            )\n",
    "    def forward(self, X):\n",
    "        output = self.network(X)\n",
    "        return output\n",
    "    \n",
    "    def train_epoch(self, train_X, train_Y, criterion, optimizer):\n",
    "        output = self.forward(train_X)\n",
    "        loss = criterion(output, train_Y)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        return loss\n",
    "\n",
    "def train_model(model, train_X, train_Y, criterion, optimizer, num_epochs=num_epochs):\n",
    "    for epoch_num in range(1, num_epochs+1):\n",
    "        if epoch_num % 1000 == 0:\n",
    "            print(f\"Epoch {epoch_num}\")\n",
    "        loss = model.train_epoch(train_X, train_Y, criterion, optimizer)\n",
    "        if epoch_num % 1000 == 0:\n",
    "            print(loss.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tensors(feature_df, vpi=False):\n",
    "    X = []\n",
    "    Y = []\n",
    "    for index, data in feature_df.iterrows():\n",
    "        #x = []\n",
    "        y = []\n",
    "        x = data['beliefstate']\n",
    "        if not vpi:\n",
    "            x = np.concatenate((x, data['one_hot_action']))\n",
    "        for f in ['expected_term_reward', 'cost']:\n",
    "            x = np.append(x, data[f])\n",
    "        #for f in features:\n",
    "        #    x.append(data[f])\n",
    "        for f in ['myopic_voc', 'vpi', 'vpi_action']:\n",
    "            y.append(data[f])\n",
    "        X.append(x)\n",
    "        Y.append(y)\n",
    "    return torch.tensor(X, requires_grad=True), torch.tensor(Y, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_data(num_points, seed = None):\n",
    "    if seed is not None:\n",
    "        np.random.seed(seed)\n",
    "    for i in range(num_points):\n",
    "        env = NewMouselabEnv(gambles, attributes, reward, cost, alpha = alpha)\n",
    "        \n",
    "        #exclude terminal action while devising current state\n",
    "        possible_actions = list(env.actions())[:-1]\n",
    "        \n",
    "        num_attributes = np.random.choice(attributes)\n",
    "        num_actions = np.random.choice(attributes*gambles)\n",
    "        \n",
    "        attributes_taken = np.random.choice(possible_actions[:attributes], size = num_attributes, replace = False)\n",
    "        actions_taken = np.random.choice(possible_actions[attributes:], size = num_actions, replace = False)\n",
    "\n",
    "        actions_taken = np.concatenate((attributes_taken, actions_taken))\n",
    "        \n",
    "        for action in actions_taken:\n",
    "            env._step(action)\n",
    "        \n",
    "        a = np.array(list(env.actions()))\n",
    "        possible_actions = list(a[a < attributes])\n",
    "        \n",
    "        \n",
    "        #possible_actions.append(env.term_action)\n",
    "        \n",
    "        action = np.random.choice(possible_actions)\n",
    "        feats = env.action_features(action)\n",
    "        \n",
    "        state = np.vectorize(lambda g: expectation(g), otypes = [float])(env._state[1])\n",
    "        \n",
    "        #print(env.mus, env.vars)\n",
    "        gamble_feats = env.mus\n",
    "        \n",
    "        yield (np.concatenate((env.dist, gamble_feats, state)), np.sort(actions_taken), action, *feats)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_one_hot_encoding(row):\n",
    "    one_hot_action = np.zeros(max_action)\n",
    "    one_hot_action[row.actions_taken] = 1\n",
    "    one_hot_action[row.action] = 2\n",
    "    return one_hot_action\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data = list(gen_data(num_datapoints))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data, columns = ['beliefstate', 'actions_taken', 'action', 'cost','myopic_voc', 'vpi_action', 'vpi', 'expected_term_reward'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['one_hot_action'] = df.apply(get_one_hot_encoding, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y = create_tensors(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "VPI_X, Y = create_tensors(df, vpi=True) # For VPI, we don't need the action vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "myopic_voc_approx = BMPS_Approximator(X.shape[-1], 1)\n",
    "vpi_approx = BMPS_Approximator(VPI_X.shape[-1], 1)\n",
    "vpi_action_approx = BMPS_Approximator(X.shape[-1], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Hyperparam\n",
    "learning_rate = 1e-4\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "# Optimizers\n",
    "mvoc_optimizer = torch.optim.Adam(myopic_voc_approx.parameters(), lr=learning_rate)\n",
    "vpi_action_optimizer = torch.optim.Adam(vpi_action_approx.parameters(), lr=learning_rate)\n",
    "vpi_optimizer = torch.optim.Adam(vpi_approx.parameters(), lr=learning_rate*10)\n",
    "\n",
    "# Train the networks\n",
    "train_model(vpi_approx, VPI_X.float(), Y[:, 1].unsqueeze_(1), criterion, vpi_optimizer)\n",
    "train_model(vpi_action_approx, X.float(), Y[:, 2].unsqueeze_(1), criterion, vpi_action_optimizer)\n",
    "train_model(myopic_voc_approx, X.float(), Y[:, 0].unsqueeze_(1), criterion, mvoc_optimizer)\n",
    "\n",
    "#create_dir(\"voc_models\")\n",
    "torch.save(vpi_approx.state_dict(), \"voc_models/vpibins.pth\")\n",
    "torch.save(vpi_action_approx.state_dict(), \"voc_models/vpi_action_attributesbins.pth\")\n",
    "torch.save(myopic_voc_approx.state_dict(), \"voc_models/myopic_voc_attributesbins.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "t2 = time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.011803865432739"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t2 - t1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import GPyOpt\n",
    "import GPy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_array = []\n",
    "\n",
    "def make_train_envs(num_episodes = num_episodes):\n",
    "    np.random.seed(1000)\n",
    "    for i in range(num_episodes):\n",
    "        env = NewMouselabEnv(gambles, attributes, reward, cost, alpha=alpha, sample_term_reward = True)\n",
    "        env_array.append(env)\n",
    "\n",
    "make_train_envs()        \n",
    "\n",
    "\n",
    "def blackboxfunc(W):\n",
    "    \n",
    "    #num_episodes = 5\n",
    "    \n",
    "    w1 = W[:,0]\n",
    "    w2 = W[:,1]\n",
    "    w3 = 1 - w1 - w2\n",
    "    w4 = W[:,2]\n",
    "    \n",
    "    def voc_estimate(action):\n",
    "        if action < attributes:\n",
    "            state = np.vectorize(lambda g: expectation(g), otypes = [float])(env._state[1])\n",
    "            gamble_feats = env.mus\n",
    "            vpi_x = np.concatenate((env.dist, gamble_feats, state))\n",
    "        \n",
    "            one_hot_action = np.zeros(max_action)\n",
    "            one_hot_action[actions_taken] = 1\n",
    "            one_hot_action[action] = 2\n",
    "        \n",
    "            term_reward = env.expected_term_reward()\n",
    "            x = np.concatenate((vpi_x, one_hot_action))\n",
    "            vpi_x = np.append(vpi_x, [term_reward, env.cost])\n",
    "            x = np.append(x, [term_reward, env.cost])\n",
    "        \n",
    "            X = torch.Tensor([x])\n",
    "            VPI_X = torch.Tensor([vpi_x])\n",
    "        \n",
    "            myopic_voc = myopic_voc_approx(X)[0].item()\n",
    "            vpi_action = vpi_action_approx(X)[0].item()\n",
    "            vpi = vpi_approx(VPI_X)[0].item()\n",
    "        \n",
    "            return w1*myopic_voc + env.cost + w2*vpi + w3*vpi_action + w4*env.cost*(len(possible_actions) - 2)\n",
    "        \n",
    "        elif action < env.term_action:\n",
    "            #features = env.action_features(action)\n",
    "            #features[0] is cost(action)\n",
    "            #features[1] is myopicVOC(action) [aka VOI]\n",
    "            #features[2] is vpi_action [aka VPIsub; the value of perfect info of branch]\n",
    "            #features[3] is vpi(beliefstate)\n",
    "            #features[4] is expected term reward of current state\n",
    "            state = np.vectorize(lambda g: expectation(g), otypes = [float])(env._state[1])\n",
    "            gamble_feats = env.mus\n",
    "            vpi_x = np.concatenate((env.dist, gamble_feats, state))\n",
    "        \n",
    "            term_reward = env.expected_term_reward()\n",
    "            vpi_x = np.append(vpi_x, [term_reward, env.cost])\n",
    "            VPI_X = torch.Tensor([vpi_x])\n",
    "            vpi = vpi_approx(VPI_X)[0].item()\n",
    "            \n",
    "            myopic_voc = env.myopic_voi(action)\n",
    "            vpi_action = env.vpi_action(action//env.outcomes)\n",
    "            \n",
    "            return w1*myopic_voc + env.cost + w2*vpi + w3*vpi_action + w4*env.cost*(len(possible_actions) - 2)\n",
    "            \n",
    "            #return w1*features[1] + w2*features[3] + w3*features[2] + w4*features[0]\n",
    "        else:\n",
    "            return 0\n",
    "        \n",
    "    cumreturn = 0\n",
    "    \n",
    "#    for i in range(num_episodes):\n",
    "#        env = NewMouselabEnv(gambles, attributes, reward, cost, alpha=0.15)\n",
    "    for env in env_array:\n",
    "\n",
    "        exp_return = 0\n",
    "        actions_taken = []\n",
    "        \n",
    "        while True:\n",
    "            possible_actions = list(env.actions())\n",
    "\n",
    "            #take action that maximises estimated VOC\n",
    "            action_taken = max(possible_actions, key = voc_estimate)\n",
    "\n",
    "            _, rew, done, _=env._step(action_taken)\n",
    "            \n",
    "            exp_return+=rew\n",
    "            actions_taken.append(action_taken)\n",
    "            \n",
    "            if done:\n",
    "                env._reset()\n",
    "                break\n",
    "        \n",
    "        cumreturn += exp_return\n",
    "        #print(exp_return)\n",
    "    \n",
    "    print(cumreturn/num_episodes)\n",
    "    return -cumreturn/num_episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pbounds = {'w1': (0,1), 'w2': (0,1), 'w3': (0,1), 'w4':(1,12)}\n",
    "space = [{'name': 'w1', 'type': 'continuous', 'domain': (0,1)},\n",
    "         {'name': 'w2', 'type': 'continuous', 'domain': (0,1)},\n",
    "         {'name': 'w4', 'type': 'continuous', 'domain': (0,1)}]\n",
    "\n",
    "constraints = [{'name': 'part_1', 'constraint': 'x[:,0] + x[:,1] - 1'}]\n",
    "\n",
    "feasible_region = GPyOpt.Design_space(space = space, constraints = constraints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- CHOOSE the intial design\n",
    "from numpy.random import seed # fixed seed\n",
    "seed(123456)\n",
    "\n",
    "initial_design = GPyOpt.experiment_design.initial_design('random', feasible_region, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- CHOOSE the objective\n",
    "objective = GPyOpt.core.task.SingleObjective(blackboxfunc)\n",
    "\n",
    "# --- CHOOSE the model type\n",
    "#This model does Maximum likelihood estimation of the hyper-parameters.\n",
    "model = GPyOpt.models.GPModel(exact_feval=True,optimize_restarts=10,verbose=False)\n",
    "\n",
    "# --- CHOOSE the acquisition optimizer\n",
    "aquisition_optimizer = GPyOpt.optimization.AcquisitionOptimizer(feasible_region)\n",
    "\n",
    "# --- CHOOSE the type of acquisition\n",
    "acquisition = GPyOpt.acquisitions.AcquisitionEI(model, feasible_region, optimizer=aquisition_optimizer)\n",
    "\n",
    "# --- CHOOSE a collection method\n",
    "evaluator = GPyOpt.core.evaluators.Sequential(acquisition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "bo = GPyOpt.methods.ModularBayesianOptimization(model, feasible_region, objective, acquisition, evaluator, initial_design)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Stop conditions\n",
    "max_time  = 60000 \n",
    "tolerance = 1e-4    # distance between two consecutive observations  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.5165312985455275\n",
      "-0.5165312985455275\n",
      "-0.5165312985455275\n",
      "-0.5165312985455275\n",
      "-0.5165312985455275\n",
      "-0.5165312985455275\n",
      "-0.5165312985455275\n",
      "-0.5165312985455275\n",
      "-0.5165312985455275\n",
      "-0.5165312985455275\n",
      "-0.5165312985455275\n",
      "num acquisition: 1, time elapsed: 0.73s\n"
     ]
    }
   ],
   "source": [
    "# Run the optimization  \n",
    "bt1 = time()\n",
    "max_iter  = 20\n",
    "bo.run_optimization(max_iter = max_iter, max_time = max_time, eps = tolerance, verbosity=True) \n",
    "bt2 = time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.127, 0.373, 0.853])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bo.x_opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "bo.plot_acquisition()\n",
    "bo.plot_convergence()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.523876428604126"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bt2 - bt1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimal_weights = bo.x_opt\n",
    "#optimal_weights = np.array([0.019, 0.91 , 3.511])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_term_reward = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testfunc(W):\n",
    "    \n",
    "    #num_episodes = 5\n",
    "    \n",
    "    w1 = W[:,0]\n",
    "    w2 = W[:,1]\n",
    "    w3 = 1 - w1 - w2\n",
    "    w4 = W[:,2]\n",
    "    \n",
    "    def voc_estimate(action):\n",
    "        if action < attributes:\n",
    "            state = np.vectorize(lambda g: expectation(g), otypes = [float])(env._state[1])\n",
    "            gamble_feats = env.mus\n",
    "            vpi_x = np.concatenate((env.dist, gamble_feats, state))\n",
    "        \n",
    "            one_hot_action = np.zeros(max_action)\n",
    "            one_hot_action[actions_taken] = 1\n",
    "            one_hot_action[action] = 2\n",
    "        \n",
    "            term_reward = env.expected_term_reward()\n",
    "            x = np.concatenate((vpi_x, one_hot_action))\n",
    "            vpi_x = np.append(vpi_x, [term_reward, env.cost])\n",
    "            x = np.append(x, [term_reward, env.cost])\n",
    "        \n",
    "            X = torch.Tensor([x])\n",
    "            VPI_X = torch.Tensor([vpi_x])\n",
    "        \n",
    "            myopic_voc = myopic_voc_approx(X)[0].item()\n",
    "            vpi_action = vpi_action_approx(X)[0].item()\n",
    "            vpi = vpi_approx(VPI_X)[0].item()\n",
    "        \n",
    "            return w1*myopic_voc + env.cost + w2*vpi + w3*vpi_action + w4*env.cost*(len(possible_actions) - 2)\n",
    "        \n",
    "        elif action < env.term_action:\n",
    "            #features = env.action_features(action)\n",
    "            #features[0] is cost(action)\n",
    "            #features[1] is myopicVOC(action) [aka VOI]\n",
    "            #features[2] is vpi_action [aka VPIsub; the value of perfect info of branch]\n",
    "            #features[3] is vpi(beliefstate)\n",
    "            #features[4] is expected term reward of current state\n",
    "            state = np.vectorize(lambda g: expectation(g), otypes = [float])(env._state[1])\n",
    "            gamble_feats = env.mus\n",
    "            vpi_x = np.concatenate((env.dist, gamble_feats, state))\n",
    "        \n",
    "            term_reward = env.expected_term_reward()\n",
    "            vpi_x = np.append(vpi_x, [term_reward, env.cost])\n",
    "            VPI_X = torch.Tensor([vpi_x])\n",
    "            vpi = vpi_approx(VPI_X)[0].item()\n",
    "            \n",
    "            myopic_voc = env.myopic_voi(action)\n",
    "            vpi_action = env.vpi_action(action//env.outcomes)\n",
    "            \n",
    "            return w1*myopic_voc + env.cost + w2*vpi + w3*vpi_action + w4*env.cost*(len(possible_actions) - 2)\n",
    "            \n",
    "            #return w1*features[1] + w2*features[3] + w3*features[2] + w4*features[0]\n",
    "        else:\n",
    "            return 0\n",
    "    \n",
    "    cumreturn = 0\n",
    "    \n",
    "    np.random.seed(100)\n",
    "    \n",
    "    for i in range(num_test_episodes):\n",
    "        env = NewMouselabEnv(gambles, attributes, reward, cost, ground_truth_dist = testreward, alpha=alpha, sample_term_reward = sample_term_reward)\n",
    "    #for env in env_array:\n",
    "\n",
    "        exp_return = 0\n",
    "        actions_taken = []\n",
    "        \n",
    "        while True:\n",
    "            possible_actions = list(env.actions())\n",
    "\n",
    "            #take action that maximises estimated VOC\n",
    "            action_taken = max(possible_actions, key = voc_estimate)\n",
    "\n",
    "            _, rew, done, _=env._step(action_taken)\n",
    "            \n",
    "            exp_return+=rew\n",
    "            actions_taken.append(action_taken)\n",
    "            \n",
    "            if done:\n",
    "                #env._reset()\n",
    "                break\n",
    "        \n",
    "        cumreturn += exp_return\n",
    "        #print(exp_return)\n",
    "    \n",
    "    print(cumreturn/num_test_episodes)\n",
    "    return -cumreturn/num_test_episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#testfunc(np.array([optimal_weights]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testsamefunc(W):\n",
    "    \n",
    "    #num_episodes = 5\n",
    "    \n",
    "    w1 = W[:,0]\n",
    "    w2 = W[:,1]\n",
    "    w3 = 1 - w1 - w2\n",
    "    w4 = W[:,2]\n",
    "    \n",
    "    def voc_estimate(action):\n",
    "        if action < attributes:\n",
    "            state = np.vectorize(lambda g: expectation(g), otypes = [float])(env._state[1])\n",
    "            gamble_feats = env.mus\n",
    "            vpi_x = np.concatenate((env.dist, gamble_feats, state))\n",
    "        \n",
    "            one_hot_action = np.zeros(max_action)\n",
    "            one_hot_action[actions_taken] = 1\n",
    "            one_hot_action[action] = 2\n",
    "        \n",
    "            term_reward = env.expected_term_reward()\n",
    "            x = np.concatenate((vpi_x, one_hot_action))\n",
    "            vpi_x = np.append(vpi_x, [term_reward, env.cost])\n",
    "            x = np.append(x, [term_reward, env.cost])\n",
    "        \n",
    "            X = torch.Tensor([x])\n",
    "            VPI_X = torch.Tensor([vpi_x])\n",
    "        \n",
    "            myopic_voc = myopic_voc_approx(X)[0].item()\n",
    "            vpi_action = vpi_action_approx(X)[0].item()\n",
    "            vpi = vpi_approx(VPI_X)[0].item()\n",
    "        \n",
    "            return w1*myopic_voc + env.cost + w2*vpi + w3*vpi_action + w4*env.cost*(len(possible_actions) - 2)\n",
    "        \n",
    "        elif action < env.term_action:\n",
    "            #features = env.action_features(action)\n",
    "            #features[0] is cost(action)\n",
    "            #features[1] is myopicVOC(action) [aka VOI]\n",
    "            #features[2] is vpi_action [aka VPIsub; the value of perfect info of branch]\n",
    "            #features[3] is vpi(beliefstate)\n",
    "            #features[4] is expected term reward of current state\n",
    "            state = np.vectorize(lambda g: expectation(g), otypes = [float])(env._state[1])\n",
    "            gamble_feats = env.mus\n",
    "            vpi_x = np.concatenate((env.dist, gamble_feats, state))\n",
    "        \n",
    "            term_reward = env.expected_term_reward()\n",
    "            vpi_x = np.append(vpi_x, [term_reward, env.cost])\n",
    "            VPI_X = torch.Tensor([vpi_x])\n",
    "            vpi = vpi_approx(VPI_X)[0].item()\n",
    "            \n",
    "            myopic_voc = env.myopic_voi(action)\n",
    "            vpi_action = env.vpi_action(action//env.outcomes)\n",
    "            \n",
    "            return w1*myopic_voc + env.cost + w2*vpi + w3*vpi_action + w4*env.cost*(len(possible_actions) - 2)\n",
    "            \n",
    "            #return w1*features[1] + w2*features[3] + w3*features[2] + w4*features[0]\n",
    "        else:\n",
    "            return 0\n",
    "    \n",
    "    cumreturn = 0\n",
    "    \n",
    "    np.random.seed(100)\n",
    "    unopened = 0\n",
    "    rewardlist = []\n",
    "    for i in range(num_test_episodes):\n",
    "        env = NewMouselabEnv(gambles, attributes, reward, cost, alpha=alpha, sample_term_reward = sample_term_reward)\n",
    "    #for env in env_array:\n",
    "\n",
    "        exp_return = 0\n",
    "        actions_taken = []\n",
    "        \n",
    "        while True:\n",
    "            possible_actions = list(env.actions())\n",
    "\n",
    "            #take action that maximises estimated VOC\n",
    "            action_taken = max(possible_actions, key = voc_estimate)\n",
    "\n",
    "            _, rew, done, _=env._step(action_taken)\n",
    "            \n",
    "            exp_return+=rew\n",
    "            actions_taken.append(action_taken)\n",
    "            \n",
    "            if done:\n",
    "                unopened += len(possible_actions) - 1 \n",
    "                #env._reset()\n",
    "                break\n",
    "        \n",
    "        cumreturn += exp_return\n",
    "        rewardlist.append(exp_return)\n",
    "        #print(exp_return)\n",
    "    \n",
    "    avgclicks = (gambles + 1)*attributes - unopened/num_test_episodes\n",
    "    print(avgclicks)\n",
    "    print(cumreturn/num_test_episodes)\n",
    "    np.save(rewardfilename, rewardlist)\n",
    "    return -cumreturn/num_test_episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "2.789088503187691\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-2.789088503187691"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testsamefunc(np.array([optimal_weights]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def observe_strategy(W, num_episodes = 5):\n",
    "    \n",
    "    #num_episodes = 5\n",
    "    \n",
    "    w1 = W[:,0]\n",
    "    w2 = W[:,1]\n",
    "    w3 = 1 - w1 - w2\n",
    "    w4 = W[:,2]\n",
    "    \n",
    "    def voc_estimate(action):\n",
    "        if action < attributes:\n",
    "            state = np.vectorize(lambda g: expectation(g), otypes = [float])(env._state[1])\n",
    "            gamble_feats = env.mus\n",
    "            vpi_x = np.concatenate((env.dist, gamble_feats, state))\n",
    "        \n",
    "            one_hot_action = np.zeros(max_action)\n",
    "            one_hot_action[actions_taken] = 1\n",
    "            one_hot_action[action] = 2\n",
    "        \n",
    "            term_reward = env.expected_term_reward()\n",
    "            x = np.concatenate((vpi_x, one_hot_action))\n",
    "            vpi_x = np.append(vpi_x, [term_reward, env.cost])\n",
    "            x = np.append(x, [term_reward, env.cost])\n",
    "        \n",
    "            X = torch.Tensor([x])\n",
    "            VPI_X = torch.Tensor([vpi_x])\n",
    "        \n",
    "            myopic_voc = myopic_voc_approx(X)[0].item()\n",
    "            vpi_action = vpi_action_approx(X)[0].item()\n",
    "            vpi = vpi_approx(VPI_X)[0].item()\n",
    "        \n",
    "            return w1*myopic_voc + env.cost + w2*vpi + w3*vpi_action + w4*env.cost*(len(possible_actions) - 2)\n",
    "        \n",
    "        elif action < env.term_action:\n",
    "            #features = env.action_features(action)\n",
    "            #features[0] is cost(action)\n",
    "            #features[1] is myopicVOC(action) [aka VOI]\n",
    "            #features[2] is vpi_action [aka VPIsub; the value of perfect info of branch]\n",
    "            #features[3] is vpi(beliefstate)\n",
    "            #features[4] is expected term reward of current state\n",
    "            state = np.vectorize(lambda g: expectation(g), otypes = [float])(env._state[1])\n",
    "            gamble_feats = env.mus\n",
    "            vpi_x = np.concatenate((env.dist, gamble_feats, state))\n",
    "        \n",
    "            term_reward = env.expected_term_reward()\n",
    "            vpi_x = np.append(vpi_x, [term_reward, env.cost])\n",
    "            VPI_X = torch.Tensor([vpi_x])\n",
    "            vpi = vpi_approx(VPI_X)[0].item()\n",
    "            \n",
    "            myopic_voc = env.myopic_voi(action)\n",
    "            vpi_action = env.vpi_action(action//env.outcomes)\n",
    "            \n",
    "            return w1*myopic_voc + env.cost + w2*vpi + w3*vpi_action + w4*env.cost*(len(possible_actions) - 2)\n",
    "            \n",
    "            #return w1*features[1] + w2*features[3] + w3*features[2] + w4*features[0]\n",
    "        else:\n",
    "            return 0\n",
    "    \n",
    "    cumreturn = 0\n",
    "    \n",
    "    np.random.seed(100)\n",
    "    unopened = 0\n",
    "    rewardlist = []\n",
    "    for i in range(num_episodes):\n",
    "        env = NewMouselabEnv(gambles, attributes, reward, cost, alpha=alpha, sample_term_reward = sample_term_reward)\n",
    "    #for env in env_array:\n",
    "\n",
    "        exp_return = 0\n",
    "        actions_taken = []\n",
    "        \n",
    "        while True:\n",
    "            possible_actions = list(env.actions())\n",
    "\n",
    "            #take action that maximises estimated VOC\n",
    "            action_taken = max(possible_actions, key = voc_estimate)\n",
    "\n",
    "            _, rew, done, _=env._step(action_taken)\n",
    "            \n",
    "            exp_return+=rew\n",
    "            actions_taken.append(action_taken)\n",
    "            print(action_taken)\n",
    "            if done:\n",
    "                unopened += len(possible_actions) - 1 \n",
    "                print(env.ground_truth, env.dist)\n",
    "                #env._reset()\n",
    "                break\n",
    "        \n",
    "        cumreturn += exp_return\n",
    "        rewardlist.append(exp_return)\n",
    "        #print(exp_return)\n",
    "    \n",
    "    avgclicks = (gambles + 1)*attributes - unopened/num_episodes\n",
    "    print(avgclicks)\n",
    "    print(cumreturn/num_episodes)\n",
    "    return -cumreturn/num_episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#observe_strategy(np.array([optimal_weights]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Myopic VOC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testMyopicVOC():\n",
    "    \n",
    "    #num_episodes = 5\n",
    "    \n",
    "    def voc_estimate(action):\n",
    "        if action < attributes:\n",
    "            state = np.vectorize(lambda g: expectation(g), otypes = [float])(env._state[1])\n",
    "            gamble_feats = env.mus\n",
    "            vpi_x = np.concatenate((env.dist, gamble_feats, state))\n",
    "        \n",
    "            one_hot_action = np.zeros(max_action)\n",
    "            one_hot_action[actions_taken] = 1\n",
    "            one_hot_action[action] = 2\n",
    "        \n",
    "            term_reward = env.expected_term_reward()\n",
    "            x = np.concatenate((vpi_x, one_hot_action))\n",
    "            x = np.append(x, [term_reward, env.cost])\n",
    "        \n",
    "            X = torch.Tensor([x])\n",
    "        \n",
    "            myopic_voc = myopic_voc_approx(X)[0].item()\n",
    "        \n",
    "            return myopic_voc + env.cost\n",
    "        \n",
    "        elif action < env.term_action:\n",
    "            myopic_voc = env.myopic_voi(action)\n",
    "            \n",
    "            return myopic_voc + env.cost\n",
    "            \n",
    "            #return w1*features[1] + w2*features[3] + w3*features[2] + w4*features[0]\n",
    "        else:\n",
    "            return 0\n",
    "    \n",
    "    cumreturn = 0\n",
    "    \n",
    "    np.random.seed(100)\n",
    "    unopened = 0\n",
    "    rewardlist = []\n",
    "    for i in range(num_test_episodes):\n",
    "        env = NewMouselabEnv(gambles, attributes, reward, cost, alpha=alpha, sample_term_reward = sample_term_reward)\n",
    "    #for env in env_array:\n",
    "\n",
    "        exp_return = 0\n",
    "        actions_taken = []\n",
    "        \n",
    "        while True:\n",
    "            possible_actions = list(env.actions())\n",
    "\n",
    "            #take action that maximises estimated VOC\n",
    "            action_taken = max(possible_actions, key = voc_estimate)\n",
    "\n",
    "            _, rew, done, _=env._step(action_taken)\n",
    "            \n",
    "            exp_return+=rew\n",
    "            actions_taken.append(action_taken)\n",
    "            \n",
    "            if done:\n",
    "                unopened += len(possible_actions) - 1 \n",
    "                #env._reset()\n",
    "                break\n",
    "        \n",
    "        cumreturn += exp_return\n",
    "        rewardlist.append(exp_return)\n",
    "        #print(exp_return)\n",
    "    \n",
    "    avgclicks = (gambles + 1)*attributes - unopened/num_test_episodes\n",
    "    print(avgclicks)\n",
    "    print(cumreturn/num_test_episodes)\n",
    "    np.save(myopicfilename, rewardlist)\n",
    "    return -cumreturn/num_test_episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "2.789088503187691\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-2.789088503187691"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testMyopicVOC()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
