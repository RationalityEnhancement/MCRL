{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from mouselabdiscrete import NewMouselabEnv\n",
    "from distributions import Normal, Mixture, PiecewiseUniform\n",
    "from time import time\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale = 100\n",
    "\n",
    "mix_pi1 = 0.9094\n",
    "mix_mu1 = 0.0069\n",
    "mix_sigma1 = 0.0298\n",
    "\n",
    "mix_pi2 = 0.0906\n",
    "mix_mu2 = -0.0307\n",
    "mix_sigma2 = 0.0907\n",
    "\n",
    "h1 = Normal(mix_mu1, mix_sigma1)\n",
    "h2 = Normal(mix_mu2, mix_sigma2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = \"mixturerewards/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import levy_stable\n",
    "\n",
    "alpha_dist = 1.2\n",
    "beta_dist = -0.75\n",
    "mu = -0.12\n",
    "sigma = 0.18\n",
    "\n",
    "def convert_to_bins_stable(alpha, beta, mu, sigma, n=10, lower = -1, upper = 1):\n",
    "    step = (upper-lower)/n\n",
    "    \n",
    "    i = lower\n",
    "    bins = []\n",
    "    probs = []\n",
    "    while True:\n",
    "        if i >=upper:\n",
    "            break\n",
    "        j = round(i + step, 2)\n",
    "        \n",
    "        bins.append((i, j))\n",
    "        if i == lower:\n",
    "            low = -np.inf\n",
    "        else:\n",
    "            low = i\n",
    "        \n",
    "        if abs(j - upper) < step/2:\n",
    "            high = np.inf\n",
    "        else:\n",
    "            high = j\n",
    "        \n",
    "        probs.append(levy_stable.cdf(high, alpha, beta, loc=mu, scale=sigma) - levy_stable.cdf(low, alpha, beta, loc=mu, scale=sigma))\n",
    "    \n",
    "        i = j \n",
    "        \n",
    "    return PiecewiseUniform(bins, probs) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "gambles = 7\n",
    "attributes = 4\n",
    "#high_stakes1 = Normal((9.99+0.01)/2, 0.3*(9.99-0.01))\n",
    "#high_stakes2 = Normal((9.99+0.01)/2, 0.9*(9.99-0.01))\n",
    "high_stakes1 = h1*scale\n",
    "high_stakes2 = h2*scale\n",
    "biasedreward = high_stakes1\n",
    "#low_stakes = Normal((0.25+0.01)/2, 0.3*(0.25-0.01))\n",
    "#reward = Mixture([high_stakes1, high_stakes2], [mix_pi1, mix_pi2])\n",
    "scaledist = 100\n",
    "scalecost = 2\n",
    "#reward = convert_to_bins_stable(alpha_dist, beta_dist, mu, sigma)*scaledist\n",
    "reward = Mixture([high_stakes1, high_stakes2], [mix_pi1, mix_pi2])\n",
    "#reward = high_stakes1\n",
    "#reward = Mixture([high_stakes1, high_stakes2], [0.7, 0.3])\n",
    "cost=0.01*scalecost\n",
    "alpha = 0.15\n",
    "sample_term_reward = True\n",
    "\n",
    "seed = 100\n",
    "test_episodes = 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#open all outcomes associated with the highest probability. TTB is practically the same as LEX since it's a continuous dist\n",
    "def TTB():\n",
    "    rewardfile = directory + \"ttbhd.npy\"\n",
    "    cumreturn = 0\n",
    "    \n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    reward_list = []\n",
    "    \n",
    "    for epno in range(test_episodes):\n",
    "        env = NewMouselabEnv(gambles, attributes, reward, cost, alpha=alpha, sample_term_reward=sample_term_reward)\n",
    "        #print(env.ground_truth)\n",
    "        exp_return = 0\n",
    "\n",
    "        for i in range(attributes - 1):\n",
    "            _, rew, _, _ = env._step(i)\n",
    "            exp_return += rew\n",
    "            if max(env.dist[:i+1]) > sum(env.dist[i+1:]):\n",
    "                break\n",
    "        \n",
    "        ind = np.argmax(env.dist)\n",
    "        for  i in range(ind + attributes, env.term_action, attributes):\n",
    "            _, rew, _, _ = env._step(i)\n",
    "            exp_return += rew\n",
    "        \n",
    "        _, rew, _, _ = env._step(env.term_action)\n",
    "        exp_return += rew\n",
    "        \n",
    "        reward_list.append(exp_return)\n",
    "        cumreturn += exp_return\n",
    "        #print(exp_return)\n",
    "    #np.save(rewardfile, reward_list)\n",
    "    print(cumreturn/test_episodes)\n",
    "    return -cumreturn/test_episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'directory' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-58956b1beba0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mTTB\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-5-7b8b1acf60c0>\u001b[0m in \u001b[0;36mTTB\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#open all outcomes associated with the highest probability. TTB is practically the same as LEX since it's a continuous dist\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mTTB\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mrewardfile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdirectory\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"ttbhd.npy\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mcumreturn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'directory' is not defined"
     ]
    }
   ],
   "source": [
    "TTB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#open all outcomes associated with the highest probability till an outcome with a sufficiently high value is observed\n",
    "def SAT_TTB(sat_val):\n",
    "    rewardfile = directory + \"satttbhd.npy\"\n",
    "    cumreturn = 0\n",
    "    \n",
    "    np.random.seed(seed)\n",
    "    reward_list = []\n",
    "    \n",
    "    for epno in range(test_episodes):\n",
    "        env = NewMouselabEnv(gambles, attributes, reward, cost, alpha=alpha, sample_term_reward=sample_term_reward)\n",
    "        #print(env.ground_truth)\n",
    "        exp_return = 0\n",
    "\n",
    "        for i in range(attributes - 1):\n",
    "            _, rew, _, _ = env._step(i)\n",
    "            exp_return += rew\n",
    "            if max(env.dist[:i+1]) > sum(env.dist[i+1:]):\n",
    "                break\n",
    "        \n",
    "        ind = np.argmax(env.dist)\n",
    "        for i in range(ind + attributes, env.term_action, attributes):\n",
    "            _, rew, _, _ = env._step(i)\n",
    "            exp_return += rew\n",
    "            if env._state[1][i - attributes] >= sat_val:\n",
    "                break\n",
    "        \n",
    "        _, rew, _, _ = env._step(env.term_action)\n",
    "        exp_return += rew\n",
    "        \n",
    "        reward_list.append(exp_return)\n",
    "        cumreturn += exp_return\n",
    "        #print(exp_return)\n",
    "    np.save(rewardfile, reward_list)\n",
    "    print(cumreturn/test_episodes)\n",
    "    return -cumreturn/test_episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.024198771682041\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-4.024198771682041"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SAT_TTB(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#terminate without opening anything\n",
    "def Random():\n",
    "    \n",
    "    cumreturn = 0\n",
    "    \n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    for epno in range(test_episodes):\n",
    "        env = NewMouselabEnv(gambles, attributes, reward, cost, alpha=alpha, sample_term_reward=sample_term_reward)\n",
    "        #print(env.ground_truth)\n",
    "        exp_return = 0\n",
    "        gamb = random.randrange(gambles)\n",
    "        rew = env.select_gamble(gamb)\n",
    "        exp_return += rew\n",
    "        \n",
    "        cumreturn += exp_return\n",
    "        #print(exp_return)\n",
    "    \n",
    "    print(cumreturn/test_episodes)\n",
    "    return -cumreturn/test_episodes\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2784776083339579\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-0.2784776083339579"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Random()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#open all outcomes of a particular gamble, if all are satisfactory, terminate; else move on to the next gamble\n",
    "def SAT(sat_val):\n",
    "    rewardfile = directory + \"sathd.npy\"\n",
    "    cumreturn = 0\n",
    "    \n",
    "    np.random.seed(seed)\n",
    "    reward_list = []\n",
    "    \n",
    "    for epno in range(test_episodes):\n",
    "        env = NewMouselabEnv(gambles, attributes, reward, cost, alpha=alpha, sample_term_reward=sample_term_reward)\n",
    "        #print(env.ground_truth)\n",
    "        exp_return = 0\n",
    "        \n",
    "        flag = 0\n",
    "        \n",
    "        for i in range(attributes, env.term_action, attributes):\n",
    "            gamble_outs = []\n",
    "            for j in range(attributes):\n",
    "                _, rew, _, _ = env._step(i + j)\n",
    "                exp_return += rew\n",
    "                gamble_outs.append(env._state[1][i+j - attributes])\n",
    "            \n",
    "            if min(gamble_outs) > sat_val:\n",
    "                rew = env.select_gamble(i//attributes - 1)\n",
    "                exp_return += rew\n",
    "                flag = 1\n",
    "                break\n",
    "                \n",
    "        #if no gamble satisfies the criterion, a random choice is made\n",
    "        if flag == 0: \n",
    "            gam = random.randrange(gambles)\n",
    "            rew = env.select_gamble(gam)\n",
    "            exp_return += rew\n",
    "            \n",
    "        reward_list.append(exp_return)\n",
    "        cumreturn += exp_return\n",
    "        #print(exp_return)\n",
    "    np.save(rewardfile, reward_list)\n",
    "    print(cumreturn/test_episodes)\n",
    "    return -cumreturn/test_episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.5478006623179963\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-1.5478006623179963"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SAT(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#open all attributes and outcomes\n",
    "def WADD():\n",
    "    rewardfile = directory + \"waddhd.npy\"\n",
    "    cumreturn = 0\n",
    "    \n",
    "    np.random.seed(seed)\n",
    "    reward_list = []\n",
    "    \n",
    "    for epno in range(test_episodes):\n",
    "        env = NewMouselabEnv(gambles, attributes, reward, cost, alpha=alpha, sample_term_reward=sample_term_reward)\n",
    "        #print(env.ground_truth)\n",
    "        exp_return = 0\n",
    "        \n",
    "        for i in range(attributes - 1):\n",
    "            _, rew, _, _ = env._step(i)\n",
    "            exp_return += rew\n",
    "        \n",
    "        for i in range(attributes, env.term_action):\n",
    "            _, rew, _, _ = env._step(i)\n",
    "            exp_return += rew\n",
    "        \n",
    "        _, rew, _, _ = env._step(env.term_action)\n",
    "        exp_return += rew\n",
    "        \n",
    "        reward_list.append(exp_return)\n",
    "        cumreturn += exp_return\n",
    "        #print(exp_return)\n",
    "    np.save(rewardfile, reward_list)\n",
    "    print(cumreturn/test_episodes)\n",
    "    return -cumreturn/test_episodes\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.827541330698848\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-3.827541330698848"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "WADD()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#open all outcomes\n",
    "def EQW():\n",
    "    rewardfile = directory + \"eqwhd.npy\"\n",
    "    cumreturn = 0\n",
    "    \n",
    "    np.random.seed(seed)\n",
    "    reward_list = []\n",
    "    \n",
    "    for epno in range(test_episodes):\n",
    "        env = NewMouselabEnv(gambles, attributes, reward, cost, alpha=alpha, sample_term_reward=sample_term_reward)\n",
    "        #print(env.ground_truth)\n",
    "        exp_return = 0\n",
    "        \n",
    "        for i in range(attributes, env.term_action):\n",
    "            _, rew, _, _ = env._step(i)\n",
    "            exp_return += rew\n",
    "        _, rew, _, _ = env._step(env.term_action)\n",
    "        exp_return += rew\n",
    "        \n",
    "        reward_list.append(exp_return)\n",
    "        cumreturn += exp_return\n",
    "        #print(exp_return)\n",
    "    np.save(rewardfile, reward_list)\n",
    "    print(cumreturn/test_episodes)\n",
    "    return -cumreturn/test_episodes\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3478093921784025\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-2.3478093921784025"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EQW()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Myopic_VOI():\n",
    "     \n",
    "    def voc_estimate(action):\n",
    "        if action >= env.term_action:\n",
    "            return 0.0\n",
    "        myopic_voi = env.myopic_voi(action)\n",
    "        return myopic_voi - env.cost\n",
    "        \n",
    "    rewardfile = directory + \"mvoihd.npy\"\n",
    "    cumreturn = 0\n",
    "    reward_list = []\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    for epno in range(test_episodes):\n",
    "        env = NewMouselabEnv(gambles, attributes, reward, cost, alpha=alpha, sample_term_reward=sample_term_reward)\n",
    "        #print(env.ground_truth)\n",
    "        exp_return = 0\n",
    "\n",
    "        while True:\n",
    "            possible_actions = list(env.actions())\n",
    "\n",
    "            #take action that maximises estimated VOC\n",
    "            action_taken = max(possible_actions, key = voc_estimate)\n",
    "            \n",
    "            #print(action_taken)\n",
    "            #if action_taken == env.term_action:\n",
    "                #print(env._state)\n",
    "                #print(env.grid())\n",
    "            _, rew, done, _=env._step(action_taken)\n",
    "            exp_return+=rew\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "        reward_list.append(exp_return)\n",
    "        cumreturn += exp_return\n",
    "        #print(exp_return)\n",
    "    np.save(rewardfile, reward_list)\n",
    "    print(cumreturn/test_episodes)\n",
    "    return -cumreturn/test_episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.8063674700960557\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-3.8063674700960557"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Myopic_VOI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Myopic_VOI_biased():\n",
    "     \n",
    "    def voc_estimate(action):\n",
    "        if action >= env.term_action:\n",
    "            return 0.0\n",
    "        myopic_voi = env.myopic_voi(action)\n",
    "        return myopic_voi - env.cost\n",
    "        \n",
    "    rewardfile = directory + \"mvoibiasedhd.npy\"\n",
    "    cumreturn = 0\n",
    "    reward_list = []\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    for epno in range(test_episodes):\n",
    "        env = NewMouselabEnv(gambles, attributes, biasedreward, cost, ground_truth_dist=reward, alpha=alpha, sample_term_reward=sample_term_reward)\n",
    "        #print(env.ground_truth)\n",
    "        exp_return = 0\n",
    "\n",
    "        while True:\n",
    "            possible_actions = list(env.actions())\n",
    "\n",
    "            #take action that maximises estimated VOC\n",
    "            action_taken = max(possible_actions, key = voc_estimate)\n",
    "            \n",
    "            #print(action_taken)\n",
    "            #if action_taken == env.term_action:\n",
    "                #print(env._state)\n",
    "                #print(env.grid())\n",
    "            _, rew, done, _=env._step(action_taken)\n",
    "            exp_return+=rew\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "        reward_list.append(exp_return)\n",
    "        cumreturn += exp_return\n",
    "        #print(exp_return)\n",
    "    np.save(rewardfile, reward_list)\n",
    "    print(cumreturn/test_episodes)\n",
    "    return -cumreturn/test_episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.8528073541220667\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-3.8528073541220667"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Myopic_VOI_biased()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compare the 2nd most probable outcome for all gambles whose most probable outcomes are at least max - JND\n",
    "#JND is passed as a parameter \n",
    "def LEXSEMI(JND):\n",
    "    \n",
    "    cumreturn = 0\n",
    "    rewardfile = directory + \"lexsemihd.npy\"\n",
    "    np.random.seed(seed)\n",
    "    reward_list = []\n",
    "    \n",
    "    for epno in range(test_episodes):\n",
    "        env = NewMouselabEnv(gambles, attributes, reward, cost, alpha=alpha, sample_term_reward=sample_term_reward)\n",
    "        #print(env.ground_truth)\n",
    "        exp_return = 0\n",
    "\n",
    "        for i in range(attributes - 1):\n",
    "            _, rew, _, _ = env._step(i)\n",
    "            exp_return += rew\n",
    "            if max(env.dist[:i+1]) > sum(env.dist[i+1:]):\n",
    "                break\n",
    "        \n",
    "        importance = np.argsort(env.dist)\n",
    "        ind = importance[-1]\n",
    "        \n",
    "        gamble_outs = []\n",
    "        \n",
    "        for  i in range(ind + attributes, env.term_action, attributes):\n",
    "            _, rew, _, _ = env._step(i)\n",
    "            exp_return += rew\n",
    "            gamble_outs.append(env._state[1][i - attributes])\n",
    "        \n",
    "        max_value = max(gamble_outs)\n",
    "        \n",
    "        gambles_to_consider = [i for i,v in enumerate(gamble_outs) if v >= max_value - JND]\n",
    "        \n",
    "        if len(gambles_to_consider) == 1:\n",
    "            rew = env.select_gamble(gambles_to_consider[0])\n",
    "            exp_return += rew\n",
    "        \n",
    "        else:\n",
    "            ind = importance[-2]\n",
    "            gamble_outs = []\n",
    "            for gamb in gambles_to_consider:\n",
    "                _, rew, _, _ = env._step(gamb*attributes + attributes + ind)\n",
    "                exp_return += rew\n",
    "                gamble_outs.append(env._state[1][gamb*attributes + ind])\n",
    "            \n",
    "            gamble_taken = gambles_to_consider[gamble_outs.index(max(gamble_outs))]\n",
    "            rew = env.select_gamble(gamble_taken)\n",
    "            exp_return += rew\n",
    "            \n",
    "        #_, rew, _, _ = env._step(env.term_action)\n",
    "        #exp_return += rew\n",
    "        \n",
    "        reward_list.append(exp_return)\n",
    "        cumreturn += exp_return\n",
    "        #print(exp_return)\n",
    "    np.save(rewardfile, reward_list)\n",
    "    print(cumreturn/test_episodes)\n",
    "    return -cumreturn/test_episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.0529703674819455\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-4.0529703674819455"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LEXSEMI(0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
