{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/amehta/virtualenvs/mouselab/lib/python3.6/site-packages/sklearn/ensemble/weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from mouselabdiscrete import NewMouselabEnv\n",
    "from evaluation import *\n",
    "from distributions import sample, expectation, Normal, Categorical, Mixture, PiecewiseUniform\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "participant_id = 0\n",
    "stock_no = 0\n",
    "\n",
    "biasedfilename = 'model_fits_dist/' + str(participant_id) + '_biased_probs.pkl'\n",
    "biasedfile = open(biasedfilename,'rb')\n",
    "biaseddists = pickle.load(biasedfile)\n",
    "\n",
    "truefilename = 'model_fits_dist/' + str(participant_id) + '_true_distributions.pkl'\n",
    "truefile = open(truefilename,'rb')\n",
    "truedists = pickle.load(truefile)\n",
    "#pickle.load('model_fits_dist/0_biased_probs.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "biased_probs = biaseddists[sorted(biaseddists.keys())[stock_no]]\n",
    "\n",
    "true_probs = truedists[sorted(truedists.keys())[stock_no]]\n",
    "\n",
    "bins = [(-1, -0.8),\n",
    " (-0.8, -0.6),\n",
    " (-0.6, -0.4),\n",
    " (-0.4, -0.2),\n",
    " (-0.2, 0.0),\n",
    " (0.0, 0.2),\n",
    " (0.2, 0.4),\n",
    " (0.4, 0.6),\n",
    " (0.6, 0.8),\n",
    " (0.8, 1.0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_datapoints = 10000\n",
    "num_epochs = 5000\n",
    "num_test_episodes = 1000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "gambles = 7\n",
    "attributes = 4\n",
    "scaledist = 100\n",
    "scalecost = 200\n",
    "#low_stakes = Normal((0.25+0.01)/2, 0.3*(0.25-0.01))\n",
    "reward = PiecewiseUniform(bins, biased_probs)*scaledist\n",
    "testreward = PiecewiseUniform(bins, true_probs)*scaledist\n",
    "cost=0.01*scalecost\n",
    "\n",
    "alpha = 0.15\n",
    "\n",
    "max_action = (gambles + 1)*attributes + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewardfilename = 'biasedrewards/mvoc' + str(scalecost) + '_' + str(participant_id) + '_' + str(stock_no) + '.npy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BMPS_Approximator(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(BMPS_Approximator, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.network = nn.Sequential(\n",
    "                            nn.Linear(input_size, input_size//2),\n",
    "                            nn.ReLU(),\n",
    "                            nn.Linear(input_size // 2, input_size // 4),\n",
    "                            nn.ReLU(),\n",
    "                            nn.Linear(input_size // 4, input_size//8),\n",
    "                            nn.ReLU(),\n",
    "                            nn.Linear(input_size // 8, output_size)\n",
    "                            )\n",
    "    def forward(self, X):\n",
    "        output = self.network(X)\n",
    "        return output\n",
    "    \n",
    "    def train_epoch(self, train_X, train_Y, criterion, optimizer):\n",
    "        output = self.forward(train_X)\n",
    "        loss = criterion(output, train_Y)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        return loss\n",
    "\n",
    "def train_model(model, train_X, train_Y, criterion, optimizer, num_epochs=num_epochs):\n",
    "    for epoch_num in range(1, num_epochs+1):\n",
    "        if epoch_num % 1000 == 0:\n",
    "            print(f\"Epoch {epoch_num}\")\n",
    "        loss = model.train_epoch(train_X, train_Y, criterion, optimizer)\n",
    "        if epoch_num % 1000 == 0:\n",
    "            print(loss.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tensors(feature_df, vpi=False):\n",
    "    X = []\n",
    "    Y = []\n",
    "    for index, data in feature_df.iterrows():\n",
    "        #x = []\n",
    "        y = []\n",
    "        x = data['beliefstate']\n",
    "        if not vpi:\n",
    "            x = np.concatenate((x, data['one_hot_action']))\n",
    "        for f in ['expected_term_reward', 'cost']:\n",
    "            x = np.append(x, data[f])\n",
    "        #for f in features:\n",
    "        #    x.append(data[f])\n",
    "        for f in ['myopic_voc']:\n",
    "            y.append(data[f])\n",
    "        X.append(x)\n",
    "        Y.append(y)\n",
    "    return torch.tensor(X, requires_grad=True), torch.tensor(Y, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_data(num_points, seed = None):\n",
    "    if seed is not None:\n",
    "        np.random.seed(seed)\n",
    "    for i in range(num_points):\n",
    "        env = NewMouselabEnv(gambles, attributes, reward, cost, alpha = alpha)\n",
    "        \n",
    "        #exclude terminal action while devising current state\n",
    "        possible_actions = list(env.actions())[:-1]\n",
    "        \n",
    "        num_attributes = np.random.choice(attributes)\n",
    "        num_actions = np.random.choice(attributes*gambles)\n",
    "        \n",
    "        attributes_taken = np.random.choice(possible_actions[:attributes], size = num_attributes, replace = False)\n",
    "        actions_taken = np.random.choice(possible_actions[attributes:], size = num_actions, replace = False)\n",
    "\n",
    "        actions_taken = np.concatenate((attributes_taken, actions_taken))\n",
    "        \n",
    "        for action in actions_taken:\n",
    "            env._step(action)\n",
    "        \n",
    "        a = np.array(list(env.actions()))\n",
    "        possible_actions = list(a[a < attributes])\n",
    "        \n",
    "        \n",
    "        #possible_actions.append(env.term_action)\n",
    "        \n",
    "        action = np.random.choice(possible_actions)\n",
    "        feats = np.array([\n",
    "                env.cost,\n",
    "                env.myopic_voi(action),\n",
    "                env.expected_term_reward()\n",
    "            ])\n",
    "        \n",
    "        state = np.vectorize(lambda g: expectation(g), otypes = [float])(env._state[1])\n",
    "        \n",
    "        #print(env.mus, env.vars)\n",
    "        gamble_feats = env.mus\n",
    "        \n",
    "        yield (np.concatenate((env.dist, gamble_feats, state)), np.sort(actions_taken), action, *feats)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_one_hot_encoding(row):\n",
    "    one_hot_action = np.zeros(max_action)\n",
    "    one_hot_action[row.actions_taken] = 1\n",
    "    one_hot_action[row.action] = 2\n",
    "    return one_hot_action\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data = list(gen_data(num_datapoints))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data, columns = ['beliefstate', 'actions_taken', 'action', 'cost','myopic_voc', 'expected_term_reward'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['one_hot_action'] = df.apply(get_one_hot_encoding, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y = create_tensors(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "myopic_voc_approx = BMPS_Approximator(X.shape[-1], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Hyperparam\n",
    "learning_rate = 1e-4\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "# Optimizers\n",
    "mvoc_optimizer = torch.optim.Adam(myopic_voc_approx.parameters(), lr=learning_rate)\n",
    "\n",
    "# Train the networks\n",
    "train_model(myopic_voc_approx, X.float(), Y[:, 0].unsqueeze_(1), criterion, mvoc_optimizer)\n",
    "\n",
    "#create_dir(\"voc_models\")\n",
    "#torch.save(myopic_voc_approx.state_dict(), \"voc_models/mvoc_attributesbins.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "t2 = time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13.116148471832275"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t2 - t1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_term_reward = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testsamefunc():\n",
    "    \n",
    "    #num_episodes = 5\n",
    "    def voc_estimate(action):\n",
    "        if action < attributes:\n",
    "            state = np.vectorize(lambda g: expectation(g), otypes = [float])(env._state[1])\n",
    "            gamble_feats = env.mus\n",
    "            vpi_x = np.concatenate((env.dist, gamble_feats, state))\n",
    "        \n",
    "            one_hot_action = np.zeros(max_action)\n",
    "            one_hot_action[actions_taken] = 1\n",
    "            one_hot_action[action] = 2\n",
    "        \n",
    "            term_reward = env.expected_term_reward()\n",
    "            x = np.concatenate((vpi_x, one_hot_action))\n",
    "            x = np.append(x, [term_reward, env.cost])\n",
    "        \n",
    "            X = torch.Tensor([x])\n",
    "        \n",
    "            myopic_voc = myopic_voc_approx(X)[0].item()\n",
    "\n",
    "            return myopic_voc + env.cost\n",
    "        \n",
    "        elif action < env.term_action:\n",
    "            \n",
    "            myopic_voc = env.myopic_voi(action)\n",
    "            return myopic_voc + env.cost\n",
    "            #return w1*features[1] + w2*features[3] + w3*features[2] + w4*features[0]\n",
    "        else:\n",
    "            return 0\n",
    "    \n",
    "    cumreturn = 0\n",
    "    \n",
    "    np.random.seed(100)\n",
    "    \n",
    "    for i in range(num_test_episodes):\n",
    "        env = NewMouselabEnv(gambles, attributes, reward, cost, alpha=alpha, sample_term_reward = sample_term_reward)\n",
    "    #for env in env_array:\n",
    "\n",
    "        exp_return = 0\n",
    "        actions_taken = []\n",
    "        \n",
    "        while True:\n",
    "            possible_actions = list(env.actions())\n",
    "\n",
    "            #take action that maximises estimated VOC\n",
    "            action_taken = max(possible_actions, key = voc_estimate)\n",
    "\n",
    "            _, rew, done, _=env._step(action_taken)\n",
    "            \n",
    "            exp_return+=rew\n",
    "            actions_taken.append(action_taken)\n",
    "            \n",
    "            if done:\n",
    "                #env._reset()\n",
    "                break\n",
    "        \n",
    "        cumreturn += exp_return\n",
    "        #print(exp_return)\n",
    "    \n",
    "    print(cumreturn/num_test_episodes)\n",
    "    return -cumreturn/num_test_episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#testsamefunc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testfunc():\n",
    "    \n",
    "    #num_episodes = 5\n",
    "    \n",
    "    \n",
    "    def voc_estimate(action):\n",
    "        if action < attributes:\n",
    "            state = np.vectorize(lambda g: expectation(g), otypes = [float])(env._state[1])\n",
    "            gamble_feats = env.mus\n",
    "            vpi_x = np.concatenate((env.dist, gamble_feats, state))\n",
    "        \n",
    "            one_hot_action = np.zeros(max_action)\n",
    "            one_hot_action[actions_taken] = 1\n",
    "            one_hot_action[action] = 2\n",
    "        \n",
    "            term_reward = env.expected_term_reward()\n",
    "            x = np.concatenate((vpi_x, one_hot_action))\n",
    "            x = np.append(x, [term_reward, env.cost])\n",
    "        \n",
    "            X = torch.Tensor([x])\n",
    "            VPI_X = torch.Tensor([vpi_x])\n",
    "        \n",
    "            myopic_voc = myopic_voc_approx(X)[0].item()\n",
    "        \n",
    "            return myopic_voc + env.cost\n",
    "        \n",
    "        elif action < env.term_action:\n",
    "            \n",
    "            myopic_voc = env.myopic_voi(action)\n",
    "            \n",
    "            return myopic_voc + env.cost\n",
    "            #return w1*features[1] + w2*features[3] + w3*features[2] + w4*features[0]\n",
    "        else:\n",
    "            return 0\n",
    "    \n",
    "    cumreturn = 0\n",
    "    \n",
    "    np.random.seed(100)\n",
    "    unopened = 0\n",
    "    rewardlist = []\n",
    "    for i in range(num_test_episodes):\n",
    "        env = NewMouselabEnv(gambles, attributes, reward, cost, ground_truth_dist = testreward, alpha=alpha, sample_term_reward = sample_term_reward)\n",
    "    #for env in env_array:\n",
    "\n",
    "        exp_return = 0\n",
    "        actions_taken = []\n",
    "        \n",
    "        while True:\n",
    "            possible_actions = list(env.actions())\n",
    "\n",
    "            #take action that maximises estimated VOC\n",
    "            action_taken = max(possible_actions, key = voc_estimate)\n",
    "\n",
    "            _, rew, done, _=env._step(action_taken)\n",
    "            \n",
    "            exp_return+=rew\n",
    "            actions_taken.append(action_taken)\n",
    "            \n",
    "            if done:\n",
    "                unopened += len(possible_actions) - 1 \n",
    "                #env._reset()\n",
    "                break\n",
    "        \n",
    "        cumreturn += exp_return\n",
    "        rewardlist.append(exp_return)\n",
    "        #print(exp_return)\n",
    "    \n",
    "    avgclicks = (gambles + 1)*attributes - unopened/num_test_episodes\n",
    "    print(avgclicks)\n",
    "    print(cumreturn/num_test_episodes)\n",
    "    np.save(rewardfilename, rewardlist)\n",
    "    return -cumreturn/num_test_episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15.8\n",
      "-23.948780831903\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "23.948780831903"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testfunc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def observe_strategy(num_episodes = 5):\n",
    "    \n",
    "    #num_episodes = 5\n",
    "    def voc_estimate(action):\n",
    "        if action < attributes:\n",
    "            state = np.vectorize(lambda g: expectation(g), otypes = [float])(env._state[1])\n",
    "            gamble_feats = env.mus\n",
    "            vpi_x = np.concatenate((env.dist, gamble_feats, state))\n",
    "        \n",
    "            one_hot_action = np.zeros(max_action)\n",
    "            one_hot_action[actions_taken] = 1\n",
    "            one_hot_action[action] = 2\n",
    "        \n",
    "            term_reward = env.expected_term_reward()\n",
    "            x = np.concatenate((vpi_x, one_hot_action))\n",
    "            x = np.append(x, [term_reward, env.cost])\n",
    "        \n",
    "            X = torch.Tensor([x])\n",
    "        \n",
    "            myopic_voc = myopic_voc_approx(X)[0].item()\n",
    "        \n",
    "            return myopic_voc + env.cost\n",
    "        \n",
    "        elif action < env.term_action:\n",
    "            \n",
    "            myopic_voc = env.myopic_voi(action)\n",
    "            \n",
    "            return myopic_voc + env.cost\n",
    "            #return w1*features[1] + w2*features[3] + w3*features[2] + w4*features[0]\n",
    "        else:\n",
    "            return 0\n",
    "    \n",
    "    cumreturn = 0\n",
    "    \n",
    "    np.random.seed(100)\n",
    "    unopened = 0\n",
    "    rewardlist = []\n",
    "    for i in range(num_episodes):\n",
    "        env = NewMouselabEnv(gambles, attributes, reward, cost, ground_truth_dist = testreward, alpha=alpha, sample_term_reward = sample_term_reward)\n",
    "    #for env in env_array:\n",
    "\n",
    "        exp_return = 0\n",
    "        actions_taken = []\n",
    "        \n",
    "        while True:\n",
    "            possible_actions = list(env.actions())\n",
    "\n",
    "            #take action that maximises estimated VOC\n",
    "            action_taken = max(possible_actions, key = voc_estimate)\n",
    "\n",
    "            _, rew, done, _=env._step(action_taken)\n",
    "            \n",
    "            exp_return+=rew\n",
    "            actions_taken.append(action_taken)\n",
    "            print(action_taken)\n",
    "            if done:\n",
    "                unopened += len(possible_actions) - 1 \n",
    "                print(env.ground_truth, env.dist)\n",
    "                #env._reset()\n",
    "                break\n",
    "        \n",
    "        cumreturn += exp_return\n",
    "        rewardlist.append(exp_return)\n",
    "        #print(exp_return)\n",
    "    \n",
    "    avgclicks = (gambles + 1)*attributes - unopened/num_episodes\n",
    "    print(avgclicks)\n",
    "    print(cumreturn/num_episodes)\n",
    "    return -cumreturn/num_episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#observe_strategy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
